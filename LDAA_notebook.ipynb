{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leakage Detection Algorithm Approximation\n",
    "## by Ida & Benjamin for the course Deep Learning\n",
    "\n",
    "In this notebook we will go through the workflow for the best performing model; the stacked CNN, LSTM and Dense layers.\n",
    "\n",
    "First, we load our dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import dataloader2\n",
    "from torch.utils import data\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.functional import relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we are given are hourly measurements of the output of a water supply, along with daily targets indicating if a leakage is detected or not. There are a lot of missing datapoints in the data that need to be dealed with, besides that, our data need to be converted to a format that is usefull for our models. Besides this, we also sort in our data to only consider the same things as the algorithm we are trying to approximate does. All these things is done in our dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train, targets_train, inputs_val, targets_val, inputs_test, targets_test = dataloader2.data_prep(load=True,\n",
    "                                                                                            path='Fruedal data.xlsx',\n",
    "                                                                                            day='weekday',\n",
    "                                                                                            start_time=1,\n",
    "                                                                                            end_time=6,\n",
    "                                                                                            num_days_seq = 50,\n",
    "                                                                                            t_train=0.4,\n",
    "                                                                                            t_test=0.6,\n",
    "                                                                                            clean=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load or data in a Dataset class and then we use a pytorch dataloader to prepare for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the size of the dataset\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Retrieve inputs and targets at the given index\n",
    "        X = self.inputs[index]\n",
    "        y = self.targets[index]\n",
    "\n",
    "        return X, y\n",
    "\n",
    "training_set = Dataset(np.array(inputs_train), np.array(targets_train))\n",
    "validation_set = Dataset(np.array(inputs_val), np.array(targets_val))\n",
    "test_set = Dataset(np.array(inputs_test), np.array(targets_test))\n",
    "\n",
    "print(f'We have {len(training_set)} samples in the training set.')\n",
    "print(f'We have {len(validation_set)} samples in the validation set.')\n",
    "print(f'We have {len(test_set)} samples in the test set.')\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(training_set, batch_size=50,\n",
    "                                          shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_set, batch_size=50,\n",
    "                                         shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prepare for GPU use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "def get_variable(x):\n",
    "    \"\"\" Converts tensors to cuda, if available. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cuda()\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_numpy(x):\n",
    "    \"\"\" Get numpy array for both cuda and not. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cpu().data.numpy()\n",
    "    return x.data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best performing model was found to be defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels = 1,\n",
    "                               out_channels = 10,\n",
    "                               kernel_size = 15,\n",
    "                               stride = 1)\n",
    "        \n",
    "        self.pool1 = nn.MaxPool1d(4, stride = 2)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(10)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size = 10, hidden_size = 8, num_layers = 2, bias=True, dropout=0.5)\n",
    "        \n",
    "        self.l_out = nn.Linear(in_features=8,\n",
    "                            out_features=output_size,\n",
    "                            bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # Output layer\n",
    "        x = self.conv1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.bn1(x)\n",
    "        x = relu(x)\n",
    "        x = self.pool1(x) \n",
    "        x = x.permute(2,0,1)        \n",
    "        x, (h, c) = self.lstm(x)\n",
    "        x = h[1].view(-1, 8)\n",
    "        x = relu(x)\n",
    "        x = self.l_out(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training was done in the following way. We are not interested in retraining model, also since this will yield a different result because of the initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model = False\n",
    "net = Net()\n",
    "if use_cuda:\n",
    "    net.cuda()\n",
    "\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0002, weight_decay=0.001)\n",
    "\n",
    "num_epoch = 4000\n",
    "\n",
    "# Track loss\n",
    "training_loss, validation_loss = [], []\n",
    "\n",
    "if train_model:\n",
    "\n",
    "    for epoch in range(num_epoch):  # loop over the dataset multiple times\n",
    "        epoch_training_loss = 0\n",
    "        epoch_validation_loss = 0\n",
    "\n",
    "        net.eval()\n",
    "\n",
    "        for i, data in enumerate(testloader, 0):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "\n",
    "            inputs.unsqueeze_(0)\n",
    "            inputs = inputs.permute(1,0,2)\n",
    "            # wrap them in Variable\n",
    "            inputs, labels = get_variable(Variable(inputs)), get_variable(Variable(labels))\n",
    "\n",
    "\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            epoch_validation_loss += loss.cpu().detach().numpy()\n",
    "\n",
    "        net.train()\n",
    "\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "\n",
    "            inputs.unsqueeze_(0)\n",
    "            inputs = inputs.permute(1,0,2)\n",
    "\n",
    "            # wrap them in Variable\n",
    "            inputs, labels = get_variable(Variable(inputs)), get_variable(Variable(labels))\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_training_loss += loss.cpu().detach().numpy()\n",
    "\n",
    "        training_loss.append(epoch_training_loss/len(training_set))\n",
    "        validation_loss.append(epoch_validation_loss/len(validation_set))\n",
    "\n",
    "    print('Finished Training')\n",
    "    \n",
    "    else:\n",
    "        net = torch.load('CNNLSTMstacked.ph')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look at the convergence by looking at the accuracy on the training and test data during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "# Plot training and validation loss\n",
    "epoch = np.arange(len(training_loss))\n",
    "plt.figure()\n",
    "plt.plot(epoch, training_loss, 'r', label='Training loss',)\n",
    "plt.plot(epoch, validation_loss, 'b', label='Validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch'), plt.ylabel('NLL')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at the models performance on the train and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = inputs_train.shape[0]\n",
    "outputs = np.zeros(n)\n",
    "net = net.cpu()\n",
    "for i in range(n):\n",
    "    inputs = torch.Tensor(inputs_train[i].reshape(1,-1))\n",
    "    inputs.unsqueeze_(0)\n",
    "    inputs = inputs.permute(1,0,2)\n",
    "    outputs[i] = net.forward(inputs).data.numpy()\n",
    " \n",
    "outputs = np.round(-outputs)\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Trainset predictions')\n",
    "plt.plot(targets_train[:n], 'b-')\n",
    "plt.plot(outputs, 'r-')\n",
    "plt.show()\n",
    "\n",
    "n = inputs_test.shape[0]\n",
    "outputs = np.zeros(n)\n",
    "for i in range(n):\n",
    "    inputs = torch.Tensor(inputs_test[i].reshape(1,-1))\n",
    "    inputs.unsqueeze_(0)\n",
    "    inputs = inputs.permute(1,0,2)\n",
    "    outputs[i] = net.forward(inputs).data.numpy()\n",
    " \n",
    "outputs = np.round(-outputs)\n",
    "\n",
    "plt.figure()\n",
    "plt.title('validations predictions')\n",
    "plt.plot(targets_test[:n], 'b-')\n",
    "plt.plot(outputs, 'r-')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
